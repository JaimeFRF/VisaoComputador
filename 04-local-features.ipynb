{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Feature Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "imagesDir = 'images' # Change this, according to your images' directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'corners_01.jpg'\n",
    "img = cv2.imread(os.path.join(imagesDir, filename))\n",
    "cv2.imshow(\"corners\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "og_img = img.copy() # let's copy the image to be able to use it later without having to read it again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Corner Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Harris Corner Detector](https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#gac1fc3598018010880e370e2f709b4345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "gray = np.float32(gray)\n",
    "\n",
    "neighbourhood = 2\n",
    "aperture = 3\n",
    "free_param = 0.04\n",
    "dst = cv2.cornerHarris(gray, neighbourhood, aperture, free_param)\n",
    "\n",
    "# result is dilated for marking the corners, not important\n",
    "dst = cv2.dilate(dst, None)\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image\n",
    "thr = 0.01\n",
    "\n",
    "img[dst > thr * dst.max()] = [0, 0, 255]\n",
    "\n",
    "cv2.imshow(\"Harris Corner Detection\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Shi-Tomasi Corner Detector](https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#ga1d6bb77486c8f92d79c8793ad995d541)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_corners = 200\n",
    "quality = 0.01\n",
    "mindist = 10\n",
    "corners = cv2.goodFeaturesToTrack(gray, max_corners, quality, mindist, blockSize=neighbourhood, k=free_param)\n",
    "corners = np.intp(corners)\n",
    "\n",
    "img2 = og_img.copy()\n",
    "for i in corners:\n",
    "    x, y = i.ravel()\n",
    "    cv2.circle(img2, (x, y), 3, 255, -1)\n",
    "\n",
    "cv2.imshow(\"Shi-Tomasi Corner Detection\", img2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.1**: Implement the [FAST Corner Detector](https://docs.opencv.org/4.x/df/d74/classcv_1_1FastFeatureDetector.html) to detect corners in images. Disable non maximum suppression and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast = cv2.FastFeatureDetector_create()\n",
    "\n",
    "kp = fast.detect(og_img, None)\n",
    "img2 = cv2.drawKeypoints(og_img, kp, None, color=(255,0,0))\n",
    "\n",
    "fast.setNonmaxSuppression(0)\n",
    "kp = fast.detect(img, None)\n",
    "img3 = cv2.drawKeypoints(img, kp, None, color=(255,0,0))\n",
    "\n",
    "cv2.imshow(\"First FAST Corner Detector\", img2)\n",
    "cv2.imshow(\"Second FAST Corner Detector\", img3)\n",
    "\n",
    "while True:\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.2**: Verify the impact of resizing the image before applying each of the corner detectors by:\n",
    " * Downsizing the image to 1/4 of its original size\n",
    " * Upsizing the image to twice its original size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast = cv2.FastFeatureDetector_create()\n",
    "\n",
    "def fast_corner_detection (img):\n",
    "    kp = fast.detect(img, None)\n",
    "    return cv2.drawKeypoints(img, kp, None, color=(255,0,0))\n",
    "\n",
    "smaller_image = cv2.resize(og_img, (og_img.shape[1] // 4, og_img.shape[0] // 4))\n",
    "bigger_image = cv2.resize(og_img, (og_img.shape[1] * 2, og_img.shape[0] * 2))\n",
    "\n",
    "img1 = fast_corner_detection(smaller_image)\n",
    "img2 = fast_corner_detection(bigger_image)\n",
    "\n",
    "cv2.imshow(\"First FAST Corner Detector\", img1)\n",
    "cv2.imshow(\"Second FAST Corner Detector\", img2)\n",
    "\n",
    "while True:\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Blob Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SIFT Blob Detector](https://docs.opencv.org/4.x/d7/d60/classcv_1_1SIFT.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img = cv2.imread(os.path.join(imagesDir, 'match_NotreDame_1.jpg'))\n",
    "new_img = cv2.resize(new_img, (0, 0), fx=0.4, fy=0.4)\n",
    "gray = cv2.cvtColor(new_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "cv2.imshow(\"image\", new_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Find the keypoints\n",
    "kp = sift.detect(gray, None)\n",
    "\n",
    "# Draw the keypoints (with size and orientation)\n",
    "sift_img = cv2.drawKeypoints(new_img, kp, None, (-1, -1, -1), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow(\"SIFT\", sift_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1**: Implement keypoint detection with the [Orb Blob Detector](https://docs.opencv.org/4.x/db/d95/classcv_1_1ORB.html) and compare the results with those obtained using SIFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "orb = cv2.ORB_create()\n",
    "keypoints_orb, descriptors_orb = orb.detectAndCompute(new_img, None)\n",
    "\n",
    "im_with_keypoints = cv2.drawKeypoints(new_img, keypoints_orb, None, (0, 255, 0), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow(\"Orb Blob\", im_with_keypoints)\n",
    "while True:\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Matching \n",
    "\n",
    "Check tutorial [here](https://docs.opencv.org/4.x/dc/dc3/tutorial_py_matcher.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = cv2.imread(os.path.join(imagesDir, 'match_box01a_1.png'), cv2.IMREAD_GRAYSCALE) # queryImage\n",
    "train = cv2.imread(os.path.join(imagesDir, 'match_box01a_2.png'), cv2.IMREAD_GRAYSCALE) # trainImage\n",
    "\n",
    "cv2.imshow(\"query\", query)\n",
    "cv2.imshow(\"train\", train)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match keypoints in images using:\n",
    " * Orb Detector for keypoint detection\n",
    " * Brute Force matcher with Hamming similarity (ideal for Orb Detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Find the keypoints and descriptors with ORB\n",
    "kp1, des1 = orb.detectAndCompute(query, None)\n",
    "kp2, des2 = orb.detectAndCompute(train, None)\n",
    "\n",
    "# Create a Brute Force Matcher object\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Match descriptors\n",
    "matches = bf.match(des1, des2)\n",
    "\n",
    "# Sort them in the order of their distance\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "# Draw matches\n",
    "match_output = cv2.drawMatches(query, kp1, train, kp2, matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "cv2.imshow(\"matches\", match_output)\n",
    "while True:\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1**: Replace the Orb Detector by SIFT for keypoint detection and verify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.SIFT_create()\n",
    "\n",
    "kp1, des1 = sift.detectAndCompute(query, None)\n",
    "kp2, des2 = sift.detectAndCompute(train, None)\n",
    "\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "matches = bf.match(des1, des2)\n",
    "\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "match_output = cv2.drawMatches(query, kp1, train, kp2, matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "cv2.imshow(\"matches\", match_output)\n",
    "while True:\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2**: Save the top-2 best matches for each keypoint and apply Lowe's ratio test to remove outliers.\n",
    "\n",
    "**Tips**: \n",
    "* You can find the top-k best matches for each keypoint by replacing ```match()``` with ```knnMatch()```. \n",
    "* You can draw all k matches using ```drawMatchesKnn()``` instead of ```drawMatches()```. \n",
    "* Lowe's ratio test analyses the two best matches. If the two best matches have similar distances, then both matches are removed. The best match is only saved if its distance is sufficiently smaller than the distance of the second-best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_match(query, train, detector, ratio_test=0.75):\n",
    "    kp1, des1 = detector.detectAndCompute(query, None)\n",
    "    kp2, des2 = detector.detectAndCompute(train, None)\n",
    "\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < ratio_test * n.distance:\n",
    "            good.append([m])\n",
    "\n",
    "    img_matches = cv2.drawMatchesKnn(query, kp1, train, kp2, good, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    return img_matches\n",
    "\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "img_sift = detect_and_match(query, train, sift)\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "img_orb = detect_and_match(query, train, orb)\n",
    "\n",
    "cv2.imshow(\"Lowe with SIFT\", img_sift)\n",
    "cv2.imshow(\"Lowe with ORB\", img_orb)\n",
    "while True:\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.3**: Replace the Brute Force algorithm by FLANN for keypoint matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_match(query, train, detector, ratio_test=0.75):\n",
    "    kp1, des1 = detector.detectAndCompute(query, None)\n",
    "    kp2, des2 = detector.detectAndCompute(train, None)\n",
    "\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=50)  \n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < ratio_test * n.distance:\n",
    "            good.append([m])\n",
    "\n",
    "    img_matches = cv2.drawMatchesKnn(query, kp1, train, kp2, good, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    return img_matches\n",
    "\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "img_sift = detect_and_match(query, train, sift)\n",
    "\n",
    "cv2.imshow(\"Lowe with SIFT\", img_sift)\n",
    "while True:\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.4**: Find Wally! Given an image of Wally's profile and a puzzle where Wally is hidden, apply keypoint detection and matching algorithms to find Wally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_match(query, train, detector, ratio_test=0.75):\n",
    "    kp1, des1 = detector.detectAndCompute(query, None)\n",
    "    kp2, des2 = detector.detectAndCompute(train, None)\n",
    "\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < ratio_test * n.distance:\n",
    "            good.append([m])\n",
    "\n",
    "    img_matches = cv2.drawMatchesKnn(query, kp1, train, kp2, good, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    return img_matches\n",
    "\n",
    "img_wallys_face = cv2.imread(\"images/wally.png\")\n",
    "img_find_wally = cv2.imread(\"images/find_wally.jpg\")\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "img_sift = detect_and_match(img_wallys_face, img_find_wally, sift)\n",
    "\n",
    "cv2.imshow(\"Find Wally Image\", img_sift)\n",
    "while True:\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
