{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Warping and Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "\n",
    "imagesDir = 'images' # Change this, according to your images' directory path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image\n",
    "img = cv2.imread(os.path.join(imagesDir, 'giraffe.jpg')) # Change this, according to your image's path\n",
    "\n",
    "# Resize image to facilitate visualization\n",
    "img = cv2.resize(img, (0, 0), fx = 0.4, fy = 0.4)\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affine Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select original coordinates of three points of the original image\n",
    "ori_coord = np.array([[0, 0], [img.shape[1] - 1, 0], [0, img.shape[0] - 1]]).astype(np.float32)\n",
    "\n",
    "# Select target coordinates (where the points will move to)\n",
    "tar_coord = np.array([[0, img.shape[1]*0.33], [img.shape[1]*0.85, img.shape[0]*0.25], [img.shape[1]*0.15, img.shape[0]*0.7]]).astype(np.float32)\n",
    "\n",
    "# Get affine transformation matrix\n",
    "warp_mat = cv2.getAffineTransform(ori_coord, tar_coord)\n",
    "\n",
    "# Apply transformation to the image\n",
    "warp_dst = cv2.warpAffine(img, warp_mat, (img.shape[1], img.shape[0]))\n",
    "\n",
    "# Show Image\n",
    "cv2.imshow('Warped Image', warp_dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.1: Rotate an image by [defining the rotation matrix](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#gafbbc470ce83812914a70abfb604f4326) and then applying transformation to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(h, w) = img.shape[:2]\n",
    "rotation_matrix = cv2.getRotationMatrix2D((w // 2, h // 2), 20, 1)\n",
    "rotated_image = cv2.warpAffine(img, rotation_matrix, (w, h))\n",
    "\n",
    "\n",
    "cv2.imshow('Rotated Image', rotated_image)\n",
    "while True:\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.2: Rotate an image through cv2.getAffineTransform()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = 45\n",
    "\n",
    "ori_coord = np.array([[0, 0], [img.shape[1] - 1, 0], [0, img.shape[0] - 1]]).astype(np.float32)\n",
    "tar_coord = np.array([[0, 0], [(img.shape[1] - 1) * math.cos(angle), 0], [0, (img.shape[0] - 1) * math.sin(angle)]]).astype(np.float32)\n",
    "\n",
    "warp_mat = cv2.getAffineTransform(ori_coord, tar_coord)\n",
    "warp_dst = cv2.warpAffine(img, warp_mat, (img.shape[1], img.shape[0]))\n",
    "\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "cv2.imshow('Rotated Image', rotated_image)\n",
    "while True:\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.3: Implement transformation matrices (without using getAffineTransform or getRotationMatrix2D) to achieve the following goals:\n",
    "* Apply a translation of 100 pixels to the right\n",
    "* Apply a rotation of 90 degrees \n",
    "* Scale the image to twice its size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = img.shape[:2] \n",
    "T = np.float32([[1, 0, 100], [0, 1, 0]]) \n",
    "img_translation = cv2.warpAffine(img, T, (width, height))\n",
    "\n",
    "#https://www.geeksforgeeks.org/python-opencv-getrotationmatrix2d-function/\n",
    "\n",
    "cx, cy = width // 2, height // 2\n",
    "\n",
    "angle = np.radians(90)\n",
    "cos_a = math.cos(angle)\n",
    "sin_a = math.sin(angle)\n",
    "\n",
    "R = np.float32([[cos_a,  -sin_a, cx - cx * cos_a + cy * sin_a], [sin_a, cos_a, cy - cx * sin_a - cy * cos_a]])\n",
    "img_rotated = cv2.warpAffine(img, R, (width, height))\n",
    "\n",
    "s = 2\n",
    "S = np.float32([[s,  0, 0], [0, s, 0]])\n",
    "img_scaled = cv2.warpAffine(img, S, (width, height)) \n",
    "\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "cv2.imshow('Translated Image', img_translation)\n",
    "cv2.imshow('Rotated Image', img_rotated)\n",
    "cv2.imshow('Scaled Image', img_scaled)\n",
    "while True:\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homography\n",
    "\n",
    "Example of homography using feature matching from last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "img1 = cv2.imread(os.path.join(imagesDir, 'match_box01a_1.png'), cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread(os.path.join(imagesDir, 'match_box01a_2.png'), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "cv2.imshow('Query', img1)\n",
    "cv2.imshow('Train', img2)\n",
    "while True:\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "# Apply FLAN Matcher\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks = 50)\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "# Store all the good matches as per Lowe's ratio test\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good.append(m)\n",
    "\n",
    "match_output = cv2.drawMatchesKnn(img1, kp1, img2, kp2, np.expand_dims(good, 0), None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# Obtain points corresponding to the matches in the query and train images\n",
    "query_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "train_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "\n",
    "# Obtain homography that represents the transformation from the points of the train image into the position of the query image \n",
    "M, mask = cv2.findHomography(train_pts, query_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Apply transformation to image\n",
    "warped_img = cv2.warpPerspective(img2, M, (img1.shape[1], img1.shape[0]),flags=cv2.INTER_LINEAR)\n",
    "\n",
    "cv2.imshow('Query', img1)\n",
    "cv2.imshow('Original Image', img2)\n",
    "cv2.imshow('Warped Image', warped_img)\n",
    "cv2.imshow('Feature Matching Image', match_output)\n",
    "while True:\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.4: Apply the drawMatches() function (check last week's notebook) to visualize the feature matching after removing the outliers using RANSAC. Compare the results of removing outliers using RANSAC with the Lowe's ratio test implemented last week.\n",
    "\n",
    "Hint: The output mask from cv2.findHomography() represents the inliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_match(query, train, detector, lowe=True, ratio_test=0.75):\n",
    "    kp1, des1 = detector.detectAndCompute(query, None)\n",
    "    kp2, des2 = detector.detectAndCompute(train, None)\n",
    "\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING if isinstance(detector, cv2.ORB) else cv2.NORM_L2, crossCheck=False)\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    good = []  \n",
    "\n",
    "    if lowe:\n",
    "        for m, n in matches:\n",
    "            if m.distance < ratio_test * n.distance:\n",
    "                good.append(m)  \n",
    "    else:\n",
    "        matches = [m for m, _ in matches]\n",
    "\n",
    "        src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        if len(matches) >= 4:  # Homography requires at least 4 points\n",
    "            H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "            mask = mask.ravel()  # Flatten mask\n",
    "            good = [m for i, m in enumerate(matches) if mask[i]]\n",
    "\n",
    "    img_matches = cv2.drawMatches(query, kp1, train, kp2, good, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    return img_matches\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "img_lowe = detect_and_match(img1, img2, orb, lowe=True)\n",
    "img_ransac = detect_and_match(img1, img2, orb, lowe=False)\n",
    "cv2.imshow(\"Lowe's Ratio Test\", img_lowe)\n",
    "cv2.imshow(\"RANSAC Filtering\", img_ransac)\n",
    "\n",
    "while True:\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1.5: Draw lines around the object by:\n",
    "* Obtaining homography that transforms points from the query image to the train image\n",
    "* Applying [the perspectiveTransform function](https://docs.opencv.org/3.4/d2/de8/group__core__array.html#gad327659ac03e5fd6894b90025e6900a7) to obtain the coordinates of the object of the query image on the train image\n",
    "* Drawing lines on the train image that connect the coordinates of the object using [polylines](https://docs.opencv.org/3.4/d6/d6e/group__imgproc__draw.html#gaa3c25f9fb764b6bef791bf034f6e26f5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_and_draw_box(query, train, detector, ratio_test=0.75):\n",
    "    kp1, des1 = detector.detectAndCompute(query, None)\n",
    "    kp2, des2 = detector.detectAndCompute(train, None)\n",
    "\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING if isinstance(detector, cv2.ORB) else cv2.NORM_L2, crossCheck=False)\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    good = [m for m, n in matches if m.distance < ratio_test * n.distance]\n",
    "\n",
    "    if len(good) >= 4:  # Homography needs at least 4 points\n",
    "        src_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "\n",
    "        H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "        if H is not None:\n",
    "            h, w = query.shape[:2]\n",
    "            obj_corners = np.float32([[0, 0], [w, 0], [w, h], [0, h]]).reshape(-1, 1, 2)\n",
    "\n",
    "            transformed_corners = cv2.perspectiveTransform(obj_corners, H)\n",
    "\n",
    "            train_with_box = train.copy()\n",
    "            cv2.polylines(train_with_box, [np.int32(transformed_corners)], isClosed=True, color=(0, 255, 0), thickness=3)\n",
    "\n",
    "            return train_with_box\n",
    "\n",
    "    return train  #\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "img_with_box = detect_and_draw_box(img1, img2, orb)\n",
    "\n",
    "cv2.imshow(\"Detected Object\", img_with_box)\n",
    "\n",
    "while True:\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Segmentation\n",
    "\n",
    "### Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image\n",
    "img = cv2.imread(os.path.join(imagesDir, 'sudoku.png'))\n",
    "\n",
    "# Convert to grayscale\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otsu Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply global binary threshold\n",
    "ret, th_global = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Apply binary threshold with Otsu's method\n",
    "ret, th_otsu = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "# Show images\n",
    "cv2.imshow('Global Threshold', th_global)\n",
    "cv2.imshow('Otsu Threshold', th_otsu)\n",
    "while True:\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.1: Verify the effects of blurring the image using a Gaussian filter, before applying the Otsu thresholding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply binary threshold with Otsu's method\n",
    "ret, th_otsu = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "blur = cv2.GaussianBlur(img,(5,5),0)\n",
    "ret, th_otsu2 = cv2.threshold(blur, 127, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "cv2.imshow('Otsu Threshold', th_otsu)\n",
    "cv2.imshow('Otsu Threshold after blur', th_otsu2)\n",
    "while True:\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply adaptive thresholding\n",
    "th_adaptive = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "# Show images\n",
    "cv2.imshow('Adaptive Threshold', th_adaptive)\n",
    "cv2.imshow('Otsu Threshold', th_otsu)\n",
    "cv2.imshow('Global Threshold', th_global)\n",
    "while True:\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.2: Verify the effects of blurring with filters of increasing sizes before applying the adaptive threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#, adaptive thresholding can help. Here, the algorithm determines the threshold for a pixel based on a small region around it. \n",
    "\n",
    "blur1 = cv2.GaussianBlur(img,(5,5),0)\n",
    "blur2 = cv2.GaussianBlur(img,(15,15),0)\n",
    "blur3 = cv2.GaussianBlur(img,(25,25),0)\n",
    "\n",
    "def get_adaptive_threshold(img):\n",
    "    return cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "\n",
    "th_adaptive = get_adaptive_threshold(img)\n",
    "th_adaptive2 = get_adaptive_threshold(blur1)\n",
    "th_adaptive3 = get_adaptive_threshold(blur2)\n",
    "th_adaptive4 = get_adaptive_threshold(blur3)\n",
    "\n",
    "cv2.imshow('Adaptive Threshold', th_adaptive)\n",
    "cv2.imshow('Adaptive Threshold with blur 1', th_adaptive2)\n",
    "cv2.imshow('Adaptive Threshold with blur 2', th_adaptive3)\n",
    "cv2.imshow('Adaptive Threshold with blur 3', th_adaptive4)\n",
    "\n",
    "while True:\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation with [K-Means](https://docs.opencv.org/master/d5/d38/group__core__cluster.html#ga9a34dc06c6ec9460e90860f15bcd2f88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image\n",
    "img2 = cv2.imread(os.path.join(imagesDir, 'home.jpg'))\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous shape: (384, 512, 3)\n",
      "Current shape: (196608, 3)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the image and turn its values to float\n",
    "print(f\"Previous shape: {img2.shape}\")\n",
    "\n",
    "reshaped_image = img2.reshape((-1,3))\n",
    "reshaped_image = np.float32(reshaped_image)\n",
    "\n",
    "print(f\"Current shape: {reshaped_image.shape}\")\n",
    "\n",
    "# Define criteria and number of clusters (k)\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "k = 4\n",
    "\n",
    "# Apply K-means\n",
    "ret, label, center = cv2.kmeans(reshaped_image, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to uint8, and make resulting image\n",
    "center = np.uint8(center)\n",
    "result = center[label.flatten()]\n",
    "result = result.reshape((img2.shape))\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img2)\n",
    "cv2.imshow('K-Means Result', result)\n",
    "while True:\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.3: Experiment with different number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_different_cluter(k):\n",
    "    ret, label, center = cv2.kmeans(reshaped_image, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    center = np.uint8(center)\n",
    "    return center[label.flatten()].reshape((img2.shape))\n",
    "\n",
    "res2 = try_different_cluter(2)\n",
    "res = try_different_cluter(4)\n",
    "res1 = try_different_cluter(8)\n",
    "\n",
    "cv2.imshow('2 clusters', res2)\n",
    "cv2.imshow('4 clusters', res)\n",
    "cv2.imshow('8 clusters', res1)\n",
    "while True:\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation with [GrabCut](https://docs.opencv.org/4.x/d3/d47/group__imgproc__segmentation.html#ga909c1dda50efcbeaa3ce126be862b37f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image\n",
    "img = cv2.imread(os.path.join(imagesDir, 'giraffe.jpg')) # Change this, according to your image's path\n",
    "\n",
    "# Resize image to facilitate visualization\n",
    "img = cv2.resize(img, (0, 0), fx = 0.6, fy = 0.6)\n",
    "\n",
    "# Show image\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image mask for the GrabCut output with same dimensions as the image\n",
    "mask = np.zeros(img.shape[:2], np.uint8)\n",
    "\n",
    "# Define the bounding box coordinates with the object of interest: (x, y, width, heigh)\n",
    "bb = (0, 0, 400, 500)\n",
    "\n",
    "# Allocate memory for the two arrays that this algorithm internally uses for the segmentation of the foreground and background\n",
    "bgModel = np.zeros((1, 65), np.float64)\n",
    "fgModel = np.zeros((1, 65), np.float64)\n",
    "\n",
    "# Apply GrabCut\n",
    "(mask, bgModel, fgModel) = cv2.grabCut(img, mask, bb, bgModel, fgModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "# All definite background and probable background pixels are set to 0, and all definite foreground and probable foreground pixels are set to 1\n",
    "output_mask = np.where((mask == cv2.GC_BGD) | (mask == cv2.GC_PR_BGD), 0, 1)\n",
    "\n",
    "# Scale the mask from the range [0, 1] to [0, 255]\n",
    "output_mask = (output_mask * 255).astype(\"uint8\")\n",
    "\n",
    "# Apply a bitwise AND to the image using the generated mask by GrabCut to obtain the final result\n",
    "grabcut_result = cv2.bitwise_and(img, img, mask=output_mask)\n",
    "\n",
    "# Show result\n",
    "cv2.imshow('Output Mask', output_mask)\n",
    "cv2.imshow('GrabCut Result', grabcut_result)\n",
    "while True:\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.4: Select a region of interest in the image (using cv2.selectROI function) for the GrabCut algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n"
     ]
    }
   ],
   "source": [
    "# Define image mask for the GrabCut output with same dimensions as the image\n",
    "mask = np.zeros(img.shape[:2], np.uint8)\n",
    "\n",
    "bb = cv2.selectROI(\"select the area\", img)\n",
    "\n",
    "# Allocate memory for the two arrays that this algorithm internally uses for the segmentation of the foreground and background\n",
    "bgModel = np.zeros((1, 65), np.float64)\n",
    "fgModel = np.zeros((1, 65), np.float64)\n",
    "\n",
    "# Apply GrabCut\n",
    "(mask, bgModel, fgModel) = cv2.grabCut(img, mask, bb, bgModel, fgModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "# All definite background and probable background pixels are set to 0, and all definite foreground and probable foreground pixels are set to 1\n",
    "output_mask = np.where((mask == cv2.GC_BGD) | (mask == cv2.GC_PR_BGD), 0, 1)\n",
    "\n",
    "# Scale the mask from the range [0, 1] to [0, 255]\n",
    "output_mask = (output_mask * 255).astype(\"uint8\")\n",
    "\n",
    "# Apply a bitwise AND to the image using the generated mask by GrabCut to obtain the final result\n",
    "grabcut_result = cv2.bitwise_and(img, img, mask=output_mask)\n",
    "\n",
    "# Show result\n",
    "cv2.imshow('Output Mask', output_mask)\n",
    "cv2.imshow('GrabCut Result', grabcut_result)\n",
    "while True:\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
