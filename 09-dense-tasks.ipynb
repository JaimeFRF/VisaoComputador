{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOsJ5IjcAVE-"
      },
      "source": [
        "# Lab 9 Dense Tasks: Segmentation + Object Detection\n",
        "\n",
        "## Part 1: Semantic Segmentation\n",
        "\n",
        "This first part is on image segmentation, in particular semantic segmentation where we want to classify each pixel in the image as a set of predefined classes. Other types of segmentation are instance segmentation where we want to distinguish different individual targets, and panoptic segmentation where we want to do both.\n",
        "\n",
        "You may want to copy this notebook to Google Colab to use GPU for training. You should then select the GPU/TPU run-time within Google Colab.\n",
        "\n",
        "To measure the quality of our outputs, we need special metrics and therefore will use `torchmetrics`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ppo-YL4tmx1",
        "outputId": "369e0cb3-1dcf-40c1-871d-649749253cda"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics torchmetrics[detection] ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soMygqHFr_-i"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.transforms import v2 as transforms\n",
        "from torch.nn import functional as F\n",
        "import torchvision\n",
        "import torchmetrics\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import os, random, re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNvgtC5RBEFQ"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "Let's start by downloading the data. We will be using the [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIeJGWYJVSDR",
        "outputId": "1daf6956-bf31-4440-b7aa-72af78aef505"
      },
      "outputs": [],
      "source": [
        "!wget https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz\n",
        "!wget https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz\n",
        "!tar -xf images.tar.gz\n",
        "!tar -xf annotations.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmxjYLUKglqh"
      },
      "source": [
        "The data was downloaded into the notebook. You can see it in Files (to the left of the notebook) in Google Colab. The images are in the folder ```images``` and the masks in ```annotations/trimaps```.\n",
        "\n",
        "In the annotations you will notice a file named ```trainval.txt``` which contains the names of images typically used for training and validation, and ```test.txt``` which contains the images typically used for testing. Let's separate the data into training, validation and testing based on these files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjHlP3YVglqi"
      },
      "source": [
        "## Define Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdoRdGNftYLX"
      },
      "outputs": [],
      "source": [
        "class OxfordPetDataset:\n",
        "    def __init__(self, root, fold, transform=None):\n",
        "        assert fold in ['train', 'val', 'test']\n",
        "        self.root = root\n",
        "        fname = 'trainval.txt' if fold in ('train', 'val') else 'test.txt'\n",
        "        self.files = [\n",
        "            # get the filenames\n",
        "            line.split()[0] for line in open(os.path.join(root, 'annotations', fname)).readlines()\n",
        "            # use only cats to keep the dataset smaller and faster to train\n",
        "            if line.split()[2] == '1'\n",
        "        ]\n",
        "        # filter images without labels\n",
        "        self.files = [f for f in self.files if os.path.exists(os.path.join(root, 'annotations', 'xmls', f + '.xml'))]\n",
        "        if fold in ['train', 'val']:\n",
        "            random.seed(42)\n",
        "            random.shuffle(self.files)\n",
        "            i = int(0.8*len(self.files))\n",
        "            self.files = self.files[:i] if fold == 'train' else self.files[i:]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        fname = self.files[i]\n",
        "        image = torchvision.tv_tensors.Image(torchvision.io.decode_image(os.path.join(self.root, 'images', fname + '.jpg')))\n",
        "        # the mask has 3 labels [pet (1), background (2), and border (3)], we merge pet and border.\n",
        "        mask = torchvision.io.decode_image(os.path.join(self.root, 'annotations', 'trimaps', fname + '.png'), 'GRAY')\n",
        "        mask = (mask == 1) | (mask == 3)\n",
        "        mask = torchvision.tv_tensors.Mask(mask)\n",
        "        xml = open(os.path.join(self.root, 'annotations', 'xmls', fname + '.xml')).read()\n",
        "        bboxes = [{key: int(value) for key, value in re.findall(r'<(\\w+)>(\\d+)</\\1>', bbox)}\n",
        "            for bbox in re.findall(r'<bndbox>(.*?)</bndbox>', xml)]\n",
        "        bboxes = [(bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax']) for bbox in bboxes]\n",
        "        bboxes = torchvision.tv_tensors.BoundingBoxes(bboxes, format='XYXY', canvas_size=image.shape[1:])\n",
        "        if self.transform:\n",
        "            image, mask, bboxes = self.transform(image, mask, bboxes)\n",
        "        return image, mask, bboxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.ToImage(),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomCrop((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomApply([transforms.RandomAffine(30, (0.2, 0.2), (0.8, 1.2))], 0.5),\n",
        "    transforms.ToDtype(torch.float32, True),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_dataset = OxfordPetDataset('.', 'train', train_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize examples from the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image, mask, bboxes = train_dataset[0]\n",
        "plt.subplot(1, 2, 1)\n",
        "for x1, y1, x2, y2 in bboxes:\n",
        "    plt.gca().add_patch(Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='r'))\n",
        "# we previously normalized images (x-μ)/σ and so we need to un-normalize them x*σ+μ for display\n",
        "image = image.permute(1, 2, 0)*torch.tensor([[[0.229, 0.224, 0.225]]]) + torch.tensor([[[0.485, 0.456, 0.406]]])\n",
        "plt.imshow(image)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(mask[0], cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34NxyAB_glqj"
      },
      "source": [
        "Define transformations to be applied to the images and data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE2SnTPquWuc"
      },
      "outputs": [],
      "source": [
        "# Define transformations to be applied to validation data\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToImage(),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.ToDtype(torch.float32, True),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Define dataloader for validation data\n",
        "val_dataset = OxfordPetDataset('.', 'val', val_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 8\n",
        "num_workers = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=lambda x: x, num_workers=num_workers)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size, collate_fn=lambda x: x, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etItv_MNBY2a"
      },
      "source": [
        "### Segmentation Model\n",
        "\n",
        "```\n",
        "┌────────┐                            ┌────────┐\n",
        "│        │  ┌────┐            ┌────┐  │        │\n",
        "│        │  │    │  ┌─┐  ┌─┐  │    │  │        │\n",
        "│        │  │    │  └─┘  └─┘  │    │  │        │\n",
        "│        │  └────┘            └────┘  │        │\n",
        "└────────┘                            └────────┘\n",
        "|______________________||______________________|\n",
        "         Encoder              Decoder\n",
        "        (resnet18)            (ours)\n",
        "```\n",
        "\n",
        "We are going to use `resnet18` as the encoder (so training is faster), and we will then build a decoder.\n",
        "\n",
        "To build the decoder: you must build 5 blocks, each one composed of: a convolution, upsample (2x) and relu. Each one of the five block should have a convolution with sizes 512/256/128/64/32, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MySegNet(torch.nn.Module):\n",
        "    def __init__(self, out_channels):\n",
        "        super().__init__()\n",
        "        # truncate'd resnet18 (without the classifier head)\n",
        "        self.encoder = ...\n",
        "        # freeze encoder to make training faster\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        # five blocks of convolution+upsample+relu with filter sizes 512/256/128/64/32\n",
        "        # use lazy convolutions\n",
        "        ...\n",
        "        ...\n",
        "        self.out = torch.nn.LazyConv2d(out_channels, 3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply your encoder-decoder\n",
        "        ...\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you network is correct, the following code should output:\n",
        "\n",
        "```torch.Size([8, 10, 224, 224])```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = MySegNet(10)\n",
        "out = model(torch.zeros(8, 3, 224, 224))\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### U-Net:\n",
        "\n",
        "To avoid upsample artifacts, U-Net introduces skip-connections between the decoder and the respective activation map from the encoder.\n",
        "\n",
        "```\n",
        "            ┌──────────────────────────────────────────┐             \n",
        "            │                                          │             \n",
        "┌─────────┐ │           ┌──────────────────┐           │  ┌─────────┐\n",
        "│         │ │  ┌──────┐ │                  │  ┌──────┐ │  │         │\n",
        "│         │ │  │      │ │  ┌───┐     ┌───┐ ▼  │      │ ▼  │         │\n",
        "│         ├─┴─►│      ├─┴─►│   ├────►│   ├─⊕─►│      ├─⊕─►│         ├\n",
        "│         │    │      │    └───┘     └───┘    │      │    │         │\n",
        "│         │    └──────┘                       └──────┘    │         │\n",
        "└─────────┘                                               └─────────┘\n",
        "|________________________________||_________________________________|\n",
        "               Encoder                         Decoder\n",
        "              (backbone)\n",
        "```\n",
        "\n",
        "Let's change your previous model, but now add concatenations between your decoder activations and the respective encoder layers from resnet-18."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyUNet(MySegNet):\n",
        "    def forward(self, x):\n",
        "        # Copy your previous fpass, but now add concatenations between the output of each decoder layer\n",
        "        # and the respective output of the encoder layer.\n",
        "        # To make it easier, we build a list with the outputs of the encoder layers.\n",
        "        previous_activation_maps = []\n",
        "        for layer in self.encoder.children():\n",
        "            prev_shape = x.shape\n",
        "            x = layer(x)\n",
        "            if x.shape[2:] != prev_shape[2:]:\n",
        "                # width/height changed, we want to use this layer as skip connectio\n",
        "                previous_activation_maps.append(x)\n",
        "        # HERE\n",
        "        ...\n",
        "        ...\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you network is correct, the following code should output:\n",
        "\n",
        "```torch.Size([8, 10, 224, 224])```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = MyUNet(10)\n",
        "out = model(torch.zeros(8, 3, 224, 224))\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For a more complex U-Net implementation, consider using [https://github.com/milesial/Pytorch-UNet](https://github.com/milesial/Pytorch-UNet)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M5_1a5TBhvb"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Start by defining the model, the optimizer, loss and metric to evaluate the model.\n",
        "\n",
        "In this case, we will use the Jaccard Index (also known as intersection over union) as the metric. Since pytorch does not have this metric implemented, we will use the ```torchmetrics``` package. [Click here](https://lightning.ai/docs/torchmetrics/stable/) to find out more about the metrics available on torchmetrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7pkokMlWSPt",
        "outputId": "54505d54-26f1-4ca0-a332-18472d8a3904"
      },
      "outputs": [],
      "source": [
        "# Use one of the models that you have built. You may compare both.\n",
        "model = ...  # TODO\n",
        "model.to(device)  # put model in GPU\n",
        "\n",
        "# Define optimizer (e.g., AdamW)\n",
        "optimizer = ...\n",
        "epochs = 10\n",
        "\n",
        "# Define loss (e.g., binary cross entropy)\n",
        "loss_fn = ...\n",
        "\n",
        "# Define metric from torchmetrics (e.g. Jaccard Index, Dice Coefficient, Pixel Accuracy)\n",
        "metric = ...\n",
        "metric.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One training/validation epoch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def one_epoch(model, optimizer, dataloader, is_training):\n",
        "  model.train() if is_training else model.eval()\n",
        "  avg_loss = avg_metric = 0\n",
        "  for batch in dataloader:\n",
        "    images = torch.stack([b[0] for b in batch]).to(device)\n",
        "    masks = torch.stack([b[1] for b in batch]).to(device)\n",
        "    # HERE: do the forward and backward pass\n",
        "    ...\n",
        "    ...\n",
        "    avg_loss += loss.item() / len(dataloader)\n",
        "    avg_metric += metric(torch.sigmoid(preds), masks).item() / len(dataloader)\n",
        "    return avg_loss, avg_metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7STXRuzglqk"
      },
      "source": [
        "Now implement the training cycle:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_history = {'loss': [], 'metric': []}\n",
        "val_history = {'loss': [], 'metric': []}\n",
        "for epoch in range(epochs):\n",
        "  # compute train\n",
        "  avg_loss, avg_metric = one_epoch(model, optimizer, train_dataloader, True)\n",
        "  train_history['loss'].append(avg_loss)\n",
        "  train_history['metric'].append(avg_metric)\n",
        "  print(f'Epoch {epoch+1:2d}/{epochs} - Train loss: {avg_loss} - Train metric: {avg_metric}')\n",
        "  # compute validation statistics\n",
        "  avg_loss, avg_metric = one_epoch(model, optimizer, val_dataloader, False)\n",
        "  val_history['loss'].append(avg_loss)\n",
        "  val_history['metric'].append(avg_metric)\n",
        "  print(f'Epoch {epoch+1:2d}/{epochs} - Val   loss: {avg_loss} - Val   metric: {avg_metric}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLC5mHQNglql"
      },
      "source": [
        "Plot metrics and loss on training and validation sets obtained during the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtXz5ZMI2rao"
      },
      "outputs": [],
      "source": [
        "plt.subplot(2, 1, 1)\n",
        "plt.title('Cross Entropy Loss')\n",
        "plt.plot(train_history['loss'], label='train')\n",
        "plt.plot(val_history['loss'], label='val')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.title('Jaccard Index')\n",
        "plt.plot(train_history['metric'], label='train')\n",
        "plt.plot(val_history['metric'], label='val')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Up7KWSBzLf"
      },
      "source": [
        "### Visual inspection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnqf3r-BpubE"
      },
      "source": [
        "Visualize the results on only a few images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s3HmvIW22R-Q",
        "outputId": "eb7c1cf5-e862-4a71-f790-0ceade51297f"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "batch = next(iter(val_dataloader))\n",
        "images = torch.stack([b[0] for b in batch]).to(device)\n",
        "masks = torch.stack([b[1] for b in batch]).to(device)\n",
        "with torch.no_grad():\n",
        "    preds = torch.sigmoid(model(images)) >= 0.5\n",
        "\n",
        "for i in range(4):\n",
        "    plt.subplot(3, 4, i+1)\n",
        "    image = images[i].permute(1, 2, 0).cpu()*torch.tensor([[[0.229, 0.224, 0.225]]]) + torch.tensor([[[0.485, 0.456, 0.406]]])\n",
        "    plt.imshow(image)\n",
        "    plt.subplot(3, 4, i+4+1)\n",
        "    plt.imshow(preds[i].permute(1, 2, 0).cpu(), cmap='gray')\n",
        "    plt.subplot(3, 4, i+4*2+1)\n",
        "    plt.imshow(masks[i].permute(1, 2, 0).cpu(), cmap='gray')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Object Detection\n",
        "\n",
        "Three families of deep object detectors exist (by chronological order): (1) two-stage (region-based), (2) one-stage (YOLO), (3) set-based (DETR).\n",
        "\n",
        "We will focus on YOLO. This object detectors take advantage predict if each cell in the latent space corresponds to an object and, if so, what class and bounding box. Notice that the latent space is an activation map smaller than the image, therefore, each cell in that activation map corresponds to a region in the original image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "                                                  ┌────────────┐\n",
        "┌─────────┐                                  ┌───►│P(object)   │\n",
        "│         │    ┌──────┐                      │    │aka Score   │\n",
        "│         │    │      │    ┌───┐     ┌───┐   │    └────────────┘\n",
        "│         ├───►│      ├───►│   ├────►│   ├───┤    ┌────────────┐\n",
        "│         │    │      │    └───┘     └───┘   │    │Bounding box│\n",
        "│         │    └──────┘                      └───►│xc,yc,w,h   │\n",
        "└─────────┘                                       └────────────┘\n",
        "|__________________________________________|      |____________|\n",
        "                                                                \n",
        "                 Encoder                               Heads    \n",
        "                (backbone)                                      \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each activation map is 7x7 and therefore will produce 7x7 predictions for scores. We need to convert the ground-truth into the same format as the output of the network.\n",
        "\n",
        "```\n",
        "┌───────────────┐            ┌───┬───┬───┬───┐\n",
        "│               │            │ 0 │ 0 │ 0 │ 0 │\n",
        "│┌────┐         │            ├───┼───┼───┼───┤\n",
        "││    │         │            │ 1 │ 1 │ 0 │ 0 │\n",
        "││    │         ├──────────► ├───┼───┼───┼───┤\n",
        "│└────┘         │            │ 1 │ 1 │ 0 │ 0 │\n",
        "│               │            ├───┼───┼───┼───┤\n",
        "│               │            │ 0 │ 0 │ 0 │ 0 │\n",
        "└───────────────┘            └───┴───┴───┴───┘\n",
        "```\n",
        "\n",
        "And the same thing must be done for each value of the bounding box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ground_truth_to_masks(batch_bboxes, input_shape, output_shape, device):\n",
        "    scores_masks = torch.zeros(len(batch_bboxes), 1, *output_shape, dtype=bool, device=device)\n",
        "    bboxes_masks = torch.zeros(len(batch_bboxes), 4, *output_shape, dtype=float, device=device)\n",
        "    yscale = output_shape[0]/input_shape[0]\n",
        "    xscale = output_shape[1]/input_shape[1]\n",
        "    for i, bboxes in enumerate(batch_bboxes):\n",
        "        for x1, y1, x2, y2 in bboxes:\n",
        "            i1 = int(torch.floor(x1*xscale))\n",
        "            j1 = int(torch.floor(y1*yscale))\n",
        "            i2 = int(torch.ceil(x2*xscale))\n",
        "            j2 = int(torch.ceil(y2*yscale))\n",
        "            scores_masks[i, :, j1:j2, i1:i2] = 1\n",
        "            bboxes_masks[i, 0, j1:j2, i1:i2] = x1\n",
        "            bboxes_masks[i, 1, j1:j2, i1:i2] = y1\n",
        "            bboxes_masks[i, 2, j1:j2, i1:i2] = x2\n",
        "            bboxes_masks[i, 3, j1:j2, i1:i2] = y2\n",
        "    return scores_masks, bboxes_masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test. Since we have images 224x224 and an activation map (neck) of 7x7, then each region will correspond to 32 pixels in the original image (i.e., 224/7)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bboxes = [(10, 40, 100, 120), (160, 120, 220, 180)]\n",
        "scores, bboxes = ground_truth_to_masks([torch.tensor(bboxes)], (224, 224), (7, 7), 'cpu')\n",
        "print('-'*16*7)\n",
        "for j in range(7):\n",
        "    for i in range(7):\n",
        "        if scores[0, 0, j, i] == 1:\n",
        "            print(f'|{bboxes[0, 0, j, i]:3.0f},{bboxes[0, 1, j, i]:3.0f},{bboxes[0, 2, j, i]:3.0f},{bboxes[0, 3, j, i]:3.0f}', end='')\n",
        "        else:\n",
        "            print('|' + ' '*15, end='')\n",
        "    print('|')\n",
        "    print('-'*16*7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyYOLO(torch.nn.Module):\n",
        "    def __init__(self, input_shape, output_shape):\n",
        "        super().__init__()\n",
        "        # use your previous code for the encoder (truncate'd resnet-18)\n",
        "        ...\n",
        "        # freeze encoder to make training faster\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        # use lazy conv2d to build two heads:\n",
        "        # - scores: output **1** score for each region\n",
        "        # - bboxes: output **4** coordinates for region\n",
        "        # Use padding according so your output is equal in shape to the latent space.\n",
        "        self.scores_head = ...\n",
        "        self.bboxes_head = ...\n",
        "        self.yscale = output_shape[0]/input_shape[0]\n",
        "        self.xscale = output_shape[1]/input_shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc = self.encoder(x)\n",
        "        scores = torch.sigmoid(self.scores_head(enc))\n",
        "        rel_bboxes = torch.sigmoid(self.bboxes_head(enc))\n",
        "        # each convolution predicts the x-offset and y-offset within each cell, we need to\n",
        "        # convert to absolute positions.\n",
        "        # also convert from CXCYWH to XYXY like the ground-truth.\n",
        "        height, width = x.shape[2:]\n",
        "        xx = torch.arange(0, width, 1/self.xscale, device=x.device)\n",
        "        yy = torch.arange(0, height, 1/self.yscale, device=x.device)\n",
        "        xx, yy = torch.meshgrid(xx, yy, indexing='xy')\n",
        "        abs_bboxes = torch.stack((\n",
        "            rel_bboxes[:, 0]/self.xscale + xx[None] - rel_bboxes[:, 2]*width/2,\n",
        "            rel_bboxes[:, 1]/self.yscale + yy[None] - rel_bboxes[:, 3]*height/2,\n",
        "            rel_bboxes[:, 0]/self.xscale + xx[None] + rel_bboxes[:, 2]*width/2,\n",
        "            rel_bboxes[:, 1]/self.yscale + yy[None] + rel_bboxes[:, 3]*height/2,\n",
        "        ), 1)\n",
        "        return scores, abs_bboxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We will use our YOLO.\n",
        "model = MyYOLO((224, 224), (7, 7))\n",
        "model.to(device)  # put model in GPU\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters())\n",
        "epochs = 25\n",
        "\n",
        "# Define metric (mAP)\n",
        "metric = torchmetrics.detection.mean_ap.MeanAveragePrecision()\n",
        "metric.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One training/validation epoch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def one_epoch(model, optimizer, dataloader, is_training):\n",
        "  model.train() if is_training else model.eval()\n",
        "  avg_loss = avg_metric = 0\n",
        "  for batch in dataloader:\n",
        "    images = torch.stack([b[0] for b in batch]).to(device)\n",
        "    bboxes = [b[2] for b in batch]\n",
        "    # (1) call ground_truth_to_masks() to convert the bboxes to a mask\n",
        "    # (2) do the forward pass to obtain the predicted scores and bboxes\n",
        "    # (3) do the loss and backward pass: use BCE for scores + MSE for bboxes\n",
        "    ...\n",
        "    ...\n",
        "    ...\n",
        "    avg_loss += loss.item() / len(dataloader)\n",
        "    # convert ground-truth and predictions to format that torchmetrics likes\n",
        "    preds = [{'boxes': bboxes.flatten(1).T, 'scores': scores[0].flatten(), 'labels': torch.zeros(7*7, dtype=int)} for scores, bboxes in zip(scores_preds, bboxes_preds)]\n",
        "    true = [{'boxes': boxes, 'labels': torch.zeros(len(boxes), dtype=int)} for boxes in bboxes]\n",
        "    avg_metric += metric(preds, true)['map_50'].item() / len(dataloader)\n",
        "  return avg_loss, avg_metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the training cycle: (same as before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_history = {'loss': [], 'metric': []}\n",
        "val_history = {'loss': [], 'metric': []}\n",
        "for epoch in range(epochs):\n",
        "  # compute train\n",
        "  avg_loss, avg_metric = one_epoch(model, optimizer, train_dataloader, True)\n",
        "  train_history['loss'].append(avg_loss)\n",
        "  train_history['metric'].append(avg_metric)\n",
        "  print(f'Epoch {epoch+1:2d}/{epochs} - Train loss: {avg_loss} - Train metric: {avg_metric}')\n",
        "  # compute validation statistics\n",
        "  avg_loss, avg_metric = one_epoch(model, optimizer, val_dataloader, False)\n",
        "  val_history['loss'].append(avg_loss)\n",
        "  val_history['metric'].append(avg_metric)\n",
        "  print(f'Epoch {epoch+1:2d}/{epochs} - Val   loss: {avg_loss} - Val   metric: {avg_metric}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visual inspection\n",
        "\n",
        "(before NMS = non-maximum suppression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "batch = next(iter(val_dataloader))\n",
        "images = torch.stack([b[0] for b in batch]).to(device)\n",
        "bboxes = [b[2] for b in batch]\n",
        "with torch.no_grad():\n",
        "    scores_preds, bboxes_preds = model(images)\n",
        "for i in range(4):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    image = images[i].permute(1, 2, 0).cpu()*torch.tensor([[[0.229, 0.224, 0.225]]]) + torch.tensor([[[0.485, 0.456, 0.406]]])\n",
        "    plt.imshow(image)\n",
        "    image_scores = scores_preds[i, 0].flatten()\n",
        "    image_bboxes = bboxes_preds[i].flatten(1).T\n",
        "    image_bboxes = image_bboxes[image_scores >= 0.5]\n",
        "    image_scores = image_scores[image_scores >= 0.5]\n",
        "    for x1, y1, x2, y2 in image_bboxes.cpu():\n",
        "        plt.gca().add_patch(Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='r', linestyle='--'))\n",
        "    for x1, y1, x2, y2 in bboxes[i].cpu():\n",
        "        plt.gca().add_patch(Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='b'))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(after NMS. use a low minimum IoU=0.10)\n",
        "\n",
        "You may use the [NMS function](https://pytorch.org/vision/master/generated/torchvision.ops.nms.html) from pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "batch = next(iter(val_dataloader))\n",
        "images = torch.stack([b[0] for b in batch]).to(device)\n",
        "bboxes = [b[2] for b in batch]\n",
        "with torch.no_grad():\n",
        "    scores_preds, bboxes_preds = model(images)\n",
        "for i in range(4):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    image = images[i].permute(1, 2, 0).cpu()*torch.tensor([[[0.229, 0.224, 0.225]]]) + torch.tensor([[[0.485, 0.456, 0.406]]])\n",
        "    plt.imshow(image)\n",
        "    image_scores = scores_preds[i, 0].flatten()\n",
        "    image_bboxes = bboxes_preds[i].flatten(1).T\n",
        "    image_bboxes = image_bboxes[image_scores >= 0.5]\n",
        "    image_scores = image_scores[image_scores >= 0.5]\n",
        "    # HERE: add NMS line\n",
        "    ix = ...\n",
        "    image_bboxes = image_bboxes[ix]\n",
        "    for x1, y1, x2, y2 in image_bboxes.cpu():\n",
        "        plt.gca().add_patch(Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='r', linestyle='--'))\n",
        "    for x1, y1, x2, y2 in bboxes[i].cpu():\n",
        "        plt.gca().add_patch(Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='b'))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Theoretical question: is there a limit to how many objects a YOLO can detect?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Object Detection with Ultralytics YOLOv8\n",
        "\n",
        "You do not need GPU to run this notebook, since we will use an existing model and not train it.\n",
        "\n",
        "Before you start, load the ```images.zip``` file into Google Colab (by uploading it to the ```files``` section)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!unzip \"/content/images.zip\" -d \"/content/data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are many different versions of YOLO available. We will be using YOLOv8.\n",
        "\n",
        "YOLOv8 was originally proposed and implemented by Ultralytics. To work with the model, we will use the ```ultralytics``` package.\n",
        "\n",
        "Note that this package contains implementations of various models. [Click here](https://docs.ultralytics.com/models/) to find out which models are available on ```ultralytics```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load model\n",
        "\n",
        "YOLOv8 comes in different versions. [Click here](https://docs.ultralytics.com/models/yolov8/) to see available versions and their computational efficiency and predictive performance on the datasets they were trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a pretrained model\n",
        "model = YOLO('yolov8s.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create folder to save the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "folder_name = \"results\"\n",
        "os.mkdir(folder_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference\n",
        "\n",
        "To apply the model to predict bounding boxes on images, we can use the ```predict()``` function. This function can receive many different inputs, including an image loaded with cv2, the path to an image and even a path to the folder that loads the images. [Click here](https://docs.ultralytics.com/modes/predict/) for more information about the inference process using YOLOv8.\n",
        "\n",
        "We will provide the path to the folder as input to obtain predictions for all the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_image_folder = '/content/data/'\n",
        "\n",
        "# Predict bounding boxes for the images\n",
        "results = model.predict(input_image_folder)\n",
        "\n",
        "for result in results:\n",
        "    # Get bounding boxes object for bounding box outputs\n",
        "    boxes = result.boxes\n",
        "    # Save image with bounding boxes and predictions to folder\n",
        "    image_name = result.path.split(os.sep)[-1]\n",
        "    result.save(filename=os.path.join(folder_name, image_name))\n",
        "\n",
        "# Uncomment to visualize the attributes that a bounding box contains\n",
        "# print(boxes[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = plt.imread(os.path.join(folder_name, \"photo2.jpg\"))\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Altering Parameters: Confidence Threshold and Non Maximum Suppression\n",
        "\n",
        "**Exercise 1**: Increase/decrease the **confidence threshold** and compare the results.\n",
        "\n",
        "When you increase/decrease the threshold, how does the number of detected objects change? And why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "folder_name = \"results_cnf\"\n",
        "os.mkdir(folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict bounding boxes for the images\n",
        "results = model.predict(input_image_folder, conf=0.5)  # return a list of Results objects\n",
        "\n",
        "for result in results:\n",
        "    # Get bounding boxes object for bounding box outputs\n",
        "    boxes = result.boxes\n",
        "    # Save image with bounding boxes and predictions to folder\n",
        "    image_name = result.path.split(os.sep)[-1]\n",
        "    result.save(filename=os.path.join(folder_name, image_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = plt.imread(os.path.join(folder_name, \"photo2.jpg\"))\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Exercise 2**: Increase/decrease the Non Maximum Suppression (NMS) threshold and compare the results.\n",
        "\n",
        "When you increase/decrease the threshold, how does the number of detected objects change? And why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "folder_name = \"results_iou\"\n",
        "os.mkdir(folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict bounding boxes for the images\n",
        "results = model.predict(input_image_folder, iou=0.4)  # return a list of Results objects\n",
        "\n",
        "for result in results:\n",
        "    # Get bounding boxes object for bounding box outputs\n",
        "    boxes = result.boxes\n",
        "    # Save image with bounding boxes and predictions to folder\n",
        "    image_name = result.path.split(os.sep)[-1]\n",
        "    result.save(filename=os.path.join(folder_name, image_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = plt.imread(os.path.join(folder_name, \"photo2.jpg\"))\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extra exercises:\n",
        "\n",
        "**Semantic segmentation**\n",
        "1. Replace your U-Net by the more complex U-Net implementation: [https://github.com/milesial/Pytorch-UNet](https://github.com/milesial/Pytorch-UNet).\n",
        "\n",
        "**Object detection**\n",
        "1. Add class prediction (cat/dog) to your `MyYOLO` model. Hint: Start by adding that label information to the dataset class.\n",
        "2. Add multi-scale - i.e., instead of using the last 7x7 layer of resnet-18, use also the previous layers where size was 14x14 and 28x28. The activation layer 7x7 will work better for larger objects (224/7=32) while the prior activation layer 28x28 will work better for smaller objects (224/28=8)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "metadata": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
